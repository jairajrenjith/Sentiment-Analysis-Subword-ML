{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e388c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72be7c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading dataset...\n",
      "üìä Available columns: ['sentiment', 'text']\n",
      "‚úÖ Text column: text\n",
      "‚úÖ Label column: sentiment\n",
      "üìä Dataset size: 80000\n"
     ]
    }
   ],
   "source": [
    "print(\"üîπ Loading dataset...\")\n",
    "df = pd.read_csv(\"data/Sentiment_Analysis.csv\")\n",
    "\n",
    "print(\"üìä Available columns:\", list(df.columns))\n",
    "\n",
    "TEXT_CANDIDATES = [\"text\", \"review\", \"sentence\", \"comment\"]\n",
    "LABEL_CANDIDATES = [\"label\", \"sentiment\", \"polarity\", \"target\"]\n",
    "\n",
    "text_col = None\n",
    "label_col = None\n",
    "\n",
    "for c in TEXT_CANDIDATES:\n",
    "    if c in df.columns:\n",
    "        text_col = c\n",
    "        break\n",
    "\n",
    "for c in LABEL_CANDIDATES:\n",
    "    if c in df.columns:\n",
    "        label_col = c\n",
    "        break\n",
    "\n",
    "if text_col is None or label_col is None:\n",
    "    raise ValueError(\"‚ùå Could not detect text or label column\")\n",
    "\n",
    "print(f\"‚úÖ Text column: {text_col}\")\n",
    "print(f\"‚úÖ Label column: {label_col}\")\n",
    "print(f\"üìä Dataset size: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f89558",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=200):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "\n",
    "    def get_stats(self, tokens):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in tokens.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i+1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, tokens):\n",
    "        new_tokens = {}\n",
    "        bigram = \" \".join(pair)\n",
    "        replacement = \"\".join(pair)\n",
    "        for word in tokens:\n",
    "            new_tokens[word.replace(bigram, replacement)] = tokens[word]\n",
    "        return new_tokens\n",
    "\n",
    "    def train(self, texts):\n",
    "        tokens = Counter()\n",
    "        for text in texts:\n",
    "            for word in str(text).lower().split():\n",
    "                tokens[\" \".join(word) + \" </w>\"] += 1\n",
    "\n",
    "        for _ in range(self.vocab_size):\n",
    "            pairs = self.get_stats(tokens)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            tokens = self.merge_vocab(best, tokens)\n",
    "\n",
    "        vocab = set()\n",
    "        for word in tokens:\n",
    "            vocab.update(word.split())\n",
    "\n",
    "        self.vocab = {t: i for i, t in enumerate(vocab)}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        output = []\n",
    "        for word in str(text).lower().split():\n",
    "            chars = list(word) + [\"</w>\"]\n",
    "            i = 0\n",
    "            while i < len(chars):\n",
    "                j = len(chars)\n",
    "                while j > i and \"\".join(chars[i:j]) not in self.vocab:\n",
    "                    j -= 1\n",
    "                output.append(\"\".join(chars[i:j]))\n",
    "                i = j\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c965198f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Training BPE tokenizer...\n",
      "‚úÖ Vocabulary saved to tokenizer/subword_vocab.json\n",
      "\n",
      "üîπ Sample Tokenization:\n",
      "\n",
      "And here is the rap song \"African Warrior Queens\", for which ChatGPT wrote the lyrics ü§é Yes, amateur but beautiful :)\\n\\n1/1 Œû 0.1 on KO ‚öîÔ∏è link below üîä sound on https://t.co/cyHY3m2qHy\n",
      "['and</w>', 'here</w>', 'is</w>', 'the</w>', 'ra', 'p</w>', 's', 'on', 'g</w>', '\"', 'a', 'f', 'rican</w>', 'war', 'ri', 'or</w>', 'qu', 'e', 'en', 's', '\"', ',</w>', 'for</w>', 'which</w>', 'chatgpt</w>', 'w', 'ro', 'te</w>', 'the</w>', 'l', 'y', 'ric', 's</w>', 'ü§é', '</w>', 'y', 'es,</w>', 'amate', 'u', 'r</w>', 'but</w>', 'be', 'a', 'u', 'ti', 'fu', 'l</w>', ':', ')', '\\\\n', '\\\\n', '1', '/', '1', '</w>', 'Œæ', '</w>', '0', '.', '1', '</w>', 'on</w>', 'k', 'o</w>', '‚öî', 'Ô∏è', '</w>', 'lin', 'k</w>', 'below</w>', 'üîä', '</w>', 's', 'oun', 'd</w>', 'on</w>', 'h', 'tt', 'p', 's', ':', '/', '/', 't', '.', 'co', '/', 'c', 'y', 'h', 'y', '3', 'm', '2', 'q', 'h', 'y</w>']\n",
      "------------------------------------------------------------\n",
      "We asked chatGPT ,\\n\\nHow to become a successful trader?\\n\\nHere is what it thinks,\\n\\n#ChatGPT https://t.co/UKfbOqyurW\n",
      "['we</w>', 'as', 'k', 'ed</w>', 'chatgpt</w>', ',', '\\\\n', '\\\\n', 'h', 'ow</w>', 'to</w>', 'be', 'come</w>', 'a</w>', 'su', 'c', 'c', 'es', 's', 'fu', 'l</w>', 't', 'rader', '?', '\\\\n', '\\\\n', 'here</w>', 'is</w>', 'what</w>', 'it</w>', 'thin', 'k', 's', ',', '\\\\n', '\\\\n', '#', 'chatgpt</w>', 'h', 'tt', 'p', 's', ':', '/', '/', 't', '.', 'co', '/', 'u', 'k', 'f', 'bo', 'q', 'y', 'u', 'r', 'w</w>']\n",
      "------------------------------------------------------------\n",
      "When I finally manage to convince myself to peel out from under the sheets each morning the second thing I'm interested in is a good strong cup of coffee. I categorize myself as a coffee drinker as opposed to a connoisseur since I'm amenable to buying whatever happens to be on sale at the market and spending my money on boutique blends happens about as often as the pope personally dispenses contraceptives.<br /><br />Now, my palate is sufficiently sophisticated to distinguish between premium and plebian but the miserly gene invariably kicks into overdrive and inhibits forking over the cash for the former and definitely precludes an outlay for what my taste buds inform is a decent product yet well short of remarkable.<br /><br />The Marley organic medium Ethiopia Yirgacheffe is a step above valued oriented Arabica but I was reminded of an ground variety with a bitter, burned aftertaste from the most notable of those upscale caf&eacute; house coffee providers (hey, I'm cheap and a hypocrite. If someone else is paying, I'm drinking).<br /><br />I tried the pods in a variety of ways (single cup brewing; steeped like tea for five minutes; open grind percolation; microwaved to the cusp of boiling) but regardless of method with eight ounces of water I found myself less than overwhelmed, and definitely unmotivated to order more.  Drinking it with cream and sugar was less offensive than black; the additives tended to mask the burnt aftertaste I detected.<br /><br />For some this group's commitment to social responsibility may influence a purchasing decision. Unfortunately, in my case that altruistic gene is in dormant state so it's all about the taste and price rather than ethical considerations.\n",
      "['when</w>', 'i</w>', 'f', 'in', 'ally</w>', 'man', 'a', 'ge</w>', 'to</w>', 'con', 'vin', 'ce</w>', 'm', 'y', 'sel', 'f</w>', 'to</w>', 'pe', 'el</w>', 'out</w>', 'from</w>', 'un', 'der</w>', 'the</w>', 'she', 'e', 'ts</w>', 'e', 'ach</w>', 'mor', 'n', 'ing</w>', 'the</w>', 'se', 'con', 'd</w>', 'thing</w>', \"i'\", 'm</w>', 'in', 'terested</w>', 'in</w>', 'is</w>', 'a</w>', 'goo', 'd</w>', 'st', 'ron', 'g</w>', 'cu', 'p</w>', 'of</w>', 'co', 'f', 'fe', 'e', '.</w>', 'i</w>', 'cate', 'g', 'ori', 'z', 'e</w>', 'm', 'y', 'sel', 'f</w>', 'as</w>', 'a</w>', 'co', 'f', 'fe', 'e</w>', 'd', 'rin', 'k', 'er</w>', 'as</w>', 'op', 'po', 'sed</w>', 'to</w>', 'a</w>', 'con', 'n', 'o', 'i', 's', 'se', 'u', 'r</w>', 'sin', 'ce</w>', \"i'\", 'm</w>', 'amen', 'ab', 'le</w>', 'to</w>', 'bu', 'y', 'ing</w>', 'whatever</w>', 'ha', 'p', 'pen', 's</w>', 'to</w>', 'be</w>', 'on</w>', 'sale</w>', 'at</w>', 'the</w>', 'mar', 'k', 'e', 't</w>', 'and</w>', 's', 'pen', 'ding</w>', 'my</w>', 'mone', 'y</w>', 'on</w>', 'bou', 'ti', 'qu', 'e</w>', 'b', 'len', 'ds</w>', 'ha', 'p', 'pen', 's</w>', 'about</w>', 'as</w>', 'o', 'f', 'ten</w>', 'as</w>', 'the</w>', 'pope</w>', 'per', 's', 'on', 'ally</w>', 'di', 's', 'pen', 'ses</w>', 'con', 't', 'rac', 'e', 'p', 'ti', 'v', 'es', '.<br</w>', '/><br</w>', '/>', 'n', 'ow', ',</w>', 'my</w>', 'p', 'alate</w>', 'is</w>', 'su', 'f', 'f', 'ic', 'i', 'en', 't', 'ly</w>', 's', 'op', 'hi', 'sticated</w>', 'to</w>', 'di', 'stingu', 'i', 'sh', '</w>', 'be', 't', 'we', 'en</w>', 'p', 're', 'mi', 'u', 'm</w>', 'and</w>', 'p', 'le', 'b', 'i', 'an</w>', 'but</w>', 'the</w>', 'mi', 'ser', 'ly</w>', 'gene</w>', 'in', 'v', 'ari', 'ab', 'ly</w>', 'k', 'ic', 'k', 's</w>', 'in', 'to</w>', 'over', 'd', 'ri', 've</w>', 'and</w>', 'in', 'hi', 'b', 'its</w>', 'for', 'king</w>', 'over</w>', 'the</w>', 'cash', '</w>', 'for</w>', 'the</w>', 'for', 'mer</w>', 'and</w>', 'de', 'f', 'in', 'itely</w>', 'p', 're', 'c', 'l', 'u', 'des</w>', 'an</w>', 'ou', 't', 'la', 'y</w>', 'for</w>', 'what</w>', 'my</w>', 'taste</w>', 'bu', 'ds</w>', 'in', 'for', 'm</w>', 'is</w>', 'a</w>', 'de', 'c', 'en', 't</w>', 'pro', 'du', 'c', 't</w>', 'y', 'e', 't</w>', 'wel', 'l</w>', 'sh', 'or', 't</w>', 'of</w>', 're', 'mar', 'k', 'ab', 'le', '.<br</w>', '/><br</w>', '/>', 'the</w>', 'mar', 'le', 'y</w>', 'or', 'g', 'an', 'ic', '</w>', 'me', 'di', 'u', 'm</w>', 'e', 'thi', 'op', 'i', 'a</w>', 'y', 'ir', 'g', 'ache', 'f', 'fe</w>', 'is</w>', 'a</w>', 'ste', 'p</w>', 'above</w>', 'v', 'al', 'u', 'ed</w>', 'ori', 'en', 'ted</w>', 'arab', 'ica</w>', 'but</w>', 'i</w>', 'was</w>', 're', 'min', 'ded</w>', 'of</w>', 'an</w>', 'g', 'roun', 'd</w>', 'v', 'ari', 'e', 't', 'y</w>', 'with</w>', 'a</w>', 'b', 'itter', ',</w>', 'bu', 'r', 'ned</w>', 'a', 'f', 'ter', 'taste</w>', 'from</w>', 'the</w>', 'mo', 'st</w>', 'n', 'o', 'tab', 'le</w>', 'of</w>', 'th', 'o', 'se</w>', 'u', 'p', 'scale</w>', 'ca', 'f', '&', 'e', 'acu', 'te', ';', '</w>', 'h', 'ou', 'se</w>', 'co', 'f', 'fe', 'e</w>', 'provi', 'der', 's</w>', '(', 'he', 'y,</w>', \"i'\", 'm</w>', 'che', 'a', 'p</w>', 'and</w>', 'a</w>', 'h', 'y', 'po', 'c', 'rite', '.</w>', 'if</w>', 'some', 'one</w>', 'el', 'se</w>', 'is</w>', 'p', 'a', 'y', 'ing', ',</w>', \"i'\", 'm</w>', 'd', 'rin', 'k', 'ing', ')', '.<br</w>', '/><br</w>', '/>', 'i</w>', 't', 'ri', 'ed</w>', 'the</w>', 'po', 'ds</w>', 'in</w>', 'a</w>', 'v', 'ari', 'e', 't', 'y</w>', 'of</w>', 'wa', 'y', 's</w>', '(', 'sing', 'le</w>', 'cu', 'p</w>', 'b', 're', 'w', 'ing', ';', '</w>', 'ste', 'e', 'ped</w>', 'like</w>', 'te', 'a</w>', 'for</w>', 'f', 'i', 've</w>', 'min', 'u', 'tes', ';', '</w>', 'open</w>', 'g', 'rin', 'd</w>', 'per', 'colati', 'on', ';', '</w>', 'mic', 'rowa', 'v', 'ed</w>', 'to</w>', 'the</w>', 'cu', 's', 'p</w>', 'of</w>', 'bo', 'iling', ')</w>', 'but</w>', 're', 'g', 'ar', 'd', 'les', 's</w>', 'of</w>', 'me', 'th', 'o', 'd</w>', 'with</w>', 'e', 'i', 'ght</w>', 'oun', 'c', 'es</w>', 'of</w>', 'water</w>', 'i</w>', 'f', 'oun', 'd</w>', 'm', 'y', 'sel', 'f</w>', 'les', 's</w>', 'than</w>', 'over', 'whel', 'me', 'd,</w>', 'and</w>', 'de', 'f', 'in', 'itely</w>', 'un', 'mo', 'ti', 'v', 'ated</w>', 'to</w>', 'or', 'der</w>', 'more', '.</w>', 'd', 'rin', 'king</w>', 'it</w>', 'with</w>', 'c', 're', 'am</w>', 'and</w>', 'su', 'g', 'ar</w>', 'was</w>', 'les', 's</w>', 'o', 'f', 'fen', 'si', 've</w>', 'than</w>', 'b', 'lac', 'k', ';', '</w>', 'the</w>', 'ad', 'diti', 'v', 'es</w>', 'ten', 'ded</w>', 'to</w>', 'mas', 'k</w>', 'the</w>', 'bu', 'r', 'n', 't</w>', 'a', 'f', 'ter', 'taste</w>', 'i</w>', 'de', 'te', 'c', 'te', 'd', '.<br</w>', '/><br</w>', '/>', 'for</w>', 'some</w>', 'this</w>', 'g', 'rou', 'p', \"'s</w>\", 'com', 'mit', 'men', 't</w>', 'to</w>', 's', 'o', 'c', 'i', 'al</w>', 'res', 'pon', 'si', 'b', 'ilit', 'y</w>', 'ma', 'y</w>', 'in', 'f', 'l', 'u', 'en', 'ce</w>', 'a</w>', 'p', 'u', 'r', 'chasing</w>', 'de', 'c', 'i', 'si', 'on', '.</w>', 'un', 'for', 'tun', 'atel', 'y,</w>', 'in</w>', 'my</w>', 'case</w>', 'that</w>', 'al', 't', 'ru', 'i', 'stic', '</w>', 'gene</w>', 'is</w>', 'in</w>', 'dor', 'man', 't</w>', 'state</w>', 'so</w>', \"it's</w>\", 'all</w>', 'about</w>', 'the</w>', 'taste</w>', 'and</w>', 'p', 'rice</w>', 'rather</w>', 'than</w>', 'e', 'thical</w>', 'con', 'si', 'derati', 'on', 's.</w>']\n",
      "------------------------------------------------------------\n",
      "Although a healthier alternative to your typical vanilla extract, I purchased this product to add some vanilla flavor to my smoothies and it didn't seem to add any flavor. Maybe it's a better product for cooking, but not for smoothies. I tasted a little on its own and the alcohol base is what overwhelmingly stood out to me over any vanilla flavor. I'm going to keep looking. Hoping to find something natural with a big vanilla bang for my buck and little to no added chemicals. Fingers crossed!\n",
      "['al', 'th', 'ou', 'gh</w>', 'a</w>', 'he', 'al', 'thi', 'er</w>', 'al', 'ter', 'n', 'ati', 've</w>', 'to</w>', 'you', 'r</w>', 't', 'y', 'p', 'ical</w>', 'v', 'an', 'il', 'la</w>', 'ex', 't', 'rac', 't,</w>', 'i</w>', 'p', 'u', 'r', 'chased</w>', 'this</w>', 'pro', 'du', 'c', 't</w>', 'to</w>', 'ad', 'd</w>', 'some</w>', 'v', 'an', 'il', 'la</w>', 'f', 'la', 'v', 'or</w>', 'to</w>', 'my</w>', 's', 'moo', 'thi', 'es</w>', 'and</w>', 'it</w>', 'di', 'd', \"n't</w>\", 'se', 'e', 'm</w>', 'to</w>', 'ad', 'd</w>', 'an', 'y</w>', 'f', 'la', 'v', 'or', '.</w>', 'ma', 'y', 'be</w>', \"it's</w>\", 'a</w>', 'be', 'tter</w>', 'pro', 'du', 'c', 't</w>', 'for</w>', 'coo', 'k', 'ing', ',</w>', 'but</w>', 'not</w>', 'for</w>', 's', 'moo', 'thi', 'es.</w>', 'i</w>', 'tasted</w>', 'a</w>', 'litt', 'le</w>', 'on</w>', 'its</w>', 'ow', 'n</w>', 'and</w>', 'the</w>', 'al', 'co', 'h', 'ol</w>', 'b', 'ase</w>', 'is</w>', 'what</w>', 'over', 'whel', 'ming', 'ly</w>', 'st', 'oo', 'd</w>', 'out</w>', 'to</w>', 'me</w>', 'over</w>', 'an', 'y</w>', 'v', 'an', 'il', 'la</w>', 'f', 'la', 'v', 'or', '.</w>', \"i'\", 'm</w>', 'g', 'o', 'ing</w>', 'to</w>', 'k', 'e', 'e', 'p</w>', 'loo', 'k', 'ing', '.</w>', 'h', 'op', 'ing</w>', 'to</w>', 'f', 'in', 'd</w>', 'some', 'thing</w>', 'n', 'atu', 'ral</w>', 'with</w>', 'a</w>', 'b', 'i', 'g</w>', 'v', 'an', 'il', 'la</w>', 'b', 'an', 'g</w>', 'for</w>', 'my</w>', 'bu', 'c', 'k</w>', 'and</w>', 'litt', 'le</w>', 'to</w>', 'n', 'o</w>', 'ad', 'ded</w>', 'che', 'mical', 's.</w>', 'f', 'inger', 's</w>', 'c', 'ro', 's', 'se', 'd', '!</w>']\n",
      "------------------------------------------------------------\n",
      "In my latest test, I asked #ChatGPT to write me a welcome letter for our attendees for banking school. #bankingschool #AI\\n\\nDear participants,\\nWelcome to our in-person class on banking! We are excited to have you join us and look forward to helping you lea‚Ä¶https://t.co/JLAp599o5u\n",
      "['in</w>', 'my</w>', 'latest</w>', 'test,</w>', 'i</w>', 'as', 'k', 'ed</w>', '#', 'chatgpt</w>', 'to</w>', 'w', 'rite</w>', 'me</w>', 'a</w>', 'wel', 'come</w>', 'le', 'tter</w>', 'for</w>', 'ou', 'r</w>', 'atten', 'de', 'es</w>', 'for</w>', 'b', 'an', 'king</w>', 'sch', 'ool', '.</w>', '#', 'b', 'an', 'k', 'ing', 'sch', 'ool</w>', '#', 'a', 'i', '\\\\n', '\\\\n', 'de', 'ar</w>', 'p', 'ar', 'tic', 'i', 'p', 'an', 't', 's', ',', '\\\\n', 'wel', 'come</w>', 'to</w>', 'ou', 'r</w>', 'in', '-', 'per', 's', 'on</w>', 'c', 'las', 's</w>', 'on</w>', 'b', 'an', 'k', 'ing', '!</w>', 'we</w>', 'are</w>', 'ex', 'c', 'ited</w>', 'to</w>', 'have</w>', 'you</w>', 'j', 'o', 'in</w>', 'u', 's</w>', 'and</w>', 'loo', 'k</w>', 'for', 'war', 'd</w>', 'to</w>', 'hel', 'p', 'ing</w>', 'you</w>', 'le', 'a', '‚Ä¶', 'h', 'tt', 'p', 's', ':', '/', '/', 't', '.', 'co', '/', 'j', 'la', 'p', '5', '9', '9', 'o', '5', 'u', '</w>']\n",
      "------------------------------------------------------------\n",
      "Distributor: GOODTIMES home video <br /><br />Plot: A pretty high school student is marked for unrelenting terror in this suspense filled made for TV movie. Gail Osborne is new in town. She makes friends, has a boyfriend and everything seems to be going her way. That is until she gets an ominous and frightening phone call while babysitting. After more and more phone calls, she is raped. throughout most of the movie, she tries to find proof that the person did rape her.<br /><br />Audio/Video: This 1987 VHS edition from Goodtimes stinks. There are constant lines at the bottom and top of the screen.<br /><br />Extras: No extras from Goodtimes home video.<br /><br />Final thoughts: This suspense filled made for TV movie was made in 1978, so don't expect many deaths (there are none). If you can find this movie with the Worldvision home video logo on the front, then buy it. But the Goodtimes version is pretty crappy. This can be a little boring, but if you are patient, the ending is pretty good.\n",
      "['di', 'st', 'ri', 'bu', 't', 'or', ':', '</w>', 'goo', 'd', 'ti', 'mes</w>', 'h', 'ome</w>', 'vi', 'de', 'o</w>', '<br</w>', '/><br</w>', '/>', 'p', 'lo', 't', ':', '</w>', 'a</w>', 'p', 're', 'tt', 'y</w>', 'hi', 'gh</w>', 'sch', 'ool</w>', 'stu', 'den', 't</w>', 'is</w>', 'mar', 'k', 'ed</w>', 'for</w>', 'un', 'relen', 'ting</w>', 'ter', 'ror</w>', 'in</w>', 'this</w>', 'su', 's', 'pen', 'se</w>', 'fil', 'led</w>', 'made</w>', 'for</w>', 't', 'v', '</w>', 'movi', 'e', '.</w>', 'g', 'a', 'il</w>', 'o', 's', 'bor', 'ne</w>', 'is</w>', 'ne', 'w</w>', 'in</w>', 't', 'ow', 'n', '.</w>', 'she</w>', 'ma', 'k', 'es</w>', 'f', 'ri', 'en', 'd', 's,</w>', 'has</w>', 'a</w>', 'bo', 'y', 'f', 'ri', 'en', 'd</w>', 'and</w>', 'ever', 'y', 'thing</w>', 'se', 'e', 'm', 's</w>', 'to</w>', 'be</w>', 'g', 'o', 'ing</w>', 'her</w>', 'wa', 'y.</w>', 'that</w>', 'is</w>', 'un', 'til</w>', 'she</w>', 'ge', 'ts</w>', 'an</w>', 'omin', 'ou', 's</w>', 'and</w>', 'f', 'ri', 'gh', 'ten', 'ing</w>', 'p', 'h', 'one</w>', 'call</w>', 'while</w>', 'b', 'ab', 'y', 'sitting', '.</w>', 'a', 'f', 'ter</w>', 'more</w>', 'and</w>', 'more</w>', 'p', 'h', 'one</w>', 'cal', 'l', 's,</w>', 'she</w>', 'is</w>', 'ra', 'pe', 'd.</w>', 'th', 'rou', 'gh', 'out</w>', 'mo', 'st</w>', 'of</w>', 'the</w>', 'movi', 'e', ',</w>', 'she</w>', 't', 'ri', 'es</w>', 'to</w>', 'f', 'in', 'd</w>', 'proof</w>', 'that</w>', 'the</w>', 'per', 's', 'on</w>', 'di', 'd</w>', 'ra', 'pe</w>', 'her', '.<br</w>', '/><br</w>', '/>', 'a', 'u', 'di', 'o', '/', 'vi', 'de', 'o', ':', '</w>', 'this</w>', '1', '9', '8', '7', '</w>', 'v', 'h', 's</w>', 'e', 'diti', 'on</w>', 'from</w>', 'goo', 'd', 'ti', 'mes</w>', 'stin', 'k', 's.</w>', 'there</w>', 'are</w>', 'con', 'stan', 't</w>', 'lines</w>', 'at</w>', 'the</w>', 'bo', 'tt', 'om</w>', 'and</w>', 't', 'op</w>', 'of</w>', 'the</w>', 'sc', 're', 'en', '.<br</w>', '/><br</w>', '/>', 'ex', 't', 'ras', ':', '</w>', 'n', 'o</w>', 'ex', 't', 'ras</w>', 'from</w>', 'goo', 'd', 'ti', 'mes</w>', 'h', 'ome</w>', 'vi', 'de', 'o', '.<br</w>', '/><br</w>', '/>', 'f', 'in', 'al</w>', 'th', 'ou', 'gh', 't', 's', ':', '</w>', 'this</w>', 'su', 's', 'pen', 'se</w>', 'fil', 'led</w>', 'made</w>', 'for</w>', 't', 'v', '</w>', 'movie</w>', 'was</w>', 'made</w>', 'in</w>', '1', '9', '7', '8', ',</w>', 'so</w>', \"don't</w>\", 'ex', 'pe', 'c', 't</w>', 'man', 'y</w>', 'de', 'ath', 's</w>', '(', 'there</w>', 'are</w>', 'n', 'one', ')', '.</w>', 'if</w>', 'you</w>', 'can</w>', 'f', 'in', 'd</w>', 'this</w>', 'movie</w>', 'with</w>', 'the</w>', 'wor', 'l', 'd', 'vi', 'si', 'on</w>', 'h', 'ome</w>', 'vi', 'de', 'o</w>', 'lo', 'g', 'o</w>', 'on</w>', 'the</w>', 'f', 'ron', 't,</w>', 'then</w>', 'bu', 'y</w>', 'it.</w>', 'but</w>', 'the</w>', 'goo', 'd', 'ti', 'mes</w>', 'ver', 'si', 'on</w>', 'is</w>', 'p', 're', 'tt', 'y</w>', 'c', 'ra', 'p', 'p', 'y.</w>', 'this</w>', 'can</w>', 'be</w>', 'a</w>', 'litt', 'le</w>', 'boring', ',</w>', 'but</w>', 'if</w>', 'you</w>', 'are</w>', 'p', 'ati', 'en', 't,</w>', 'the</w>', 'en', 'ding</w>', 'is</w>', 'p', 're', 'tt', 'y</w>', 'goo', 'd.</w>']\n",
      "------------------------------------------------------------\n",
      "I recently started my own ice cream and snack truck and have been trying different products out. This brand is easily my favorite. The simple spout tops make it easy for me in that I don't have to worry about refilling other containers, and the flavors are great. Overall, this syrup is a great value! I'd definitely recommend it!\n",
      "['i</w>', 're', 'c', 'en', 't', 'ly</w>', 'star', 'ted</w>', 'my</w>', 'ow', 'n</w>', 'ice</w>', 'c', 're', 'am</w>', 'and</w>', 's', 'n', 'ac', 'k</w>', 't', 'ru', 'c', 'k</w>', 'and</w>', 'have</w>', 'be', 'en</w>', 't', 'r', 'y', 'ing</w>', 'di', 'f', 'feren', 't</w>', 'pro', 'du', 'c', 'ts</w>', 'ou', 't.</w>', 'this</w>', 'b', 'rand</w>', 'is</w>', 'e', 'asily</w>', 'my</w>', 'f', 'a', 'v', 'orite', '.</w>', 'the</w>', 'si', 'm', 'p', 'le</w>', 's', 'pout</w>', 't', 'op', 's</w>', 'ma', 'ke</w>', 'it</w>', 'e', 'as', 'y</w>', 'for</w>', 'me</w>', 'in</w>', 'that</w>', 'i</w>', \"don't</w>\", 'have</w>', 'to</w>', 'wor', 'r', 'y</w>', 'about</w>', 're', 'fil', 'ling</w>', 'o', 'ther</w>', 'con', 'ta', 'iner', 's,</w>', 'and</w>', 'the</w>', 'f', 'la', 'v', 'or', 's</w>', 'are</w>', 'gre', 'at.</w>', 'overal', 'l', ',</w>', 'this</w>', 's', 'y', 'ru', 'p</w>', 'is</w>', 'a</w>', 'gre', 'at</w>', 'v', 'al', 'u', 'e', '!</w>', \"i'\", 'd</w>', 'de', 'f', 'in', 'itely</w>', 're', 'com', 'men', 'd</w>', 'it', '!</w>']\n",
      "------------------------------------------------------------\n",
      "I recently began a gluten-free diet and have tried about 10 different types of cookies and bars, i even attempted to make some from scratch which ended in the garbage leaving me very frustrated. I couldn't handle the texture and taste of the rice flour or the mixes i purchased. I was about to give up on finding a good wheat cookie substitute and decided to try these, even though the packaging did not make them look appetizing at all.  To my surprise these cookies tasted like the real thing.  I am very happy and will continue purchasing these from now on whenever i feel like i need a treat and will not feel sorry for myself any more. Thanks Pamela!!\n",
      "['i</w>', 're', 'c', 'en', 't', 'ly</w>', 'be', 'g', 'an</w>', 'a</w>', 'g', 'l', 'u', 'ten', '-', 'f', 're', 'e</w>', 'di', 'e', 't</w>', 'and</w>', 'have</w>', 't', 'ri', 'ed</w>', 'about</w>', '1', '0', '</w>', 'di', 'f', 'feren', 't</w>', 't', 'y', 'pes</w>', 'of</w>', 'coo', 'k', 'i', 'es</w>', 'and</w>', 'b', 'ar', 's,</w>', 'i</w>', 'ev', 'en</w>', 'atte', 'm', 'p', 'ted</w>', 'to</w>', 'ma', 'ke</w>', 'some</w>', 'from</w>', 'sc', 'rat', 'ch</w>', 'which</w>', 'en', 'ded</w>', 'in</w>', 'the</w>', 'g', 'ar', 'b', 'a', 'ge</w>', 'le', 'a', 'ving</w>', 'me</w>', 'very</w>', 'f', 'ru', 'st', 'rate', 'd.</w>', 'i</w>', 'cou', 'l', 'd', \"n't</w>\", 'han', 'd', 'le</w>', 'the</w>', 'tex', 'tu', 're</w>', 'and</w>', 'taste</w>', 'of</w>', 'the</w>', 'rice</w>', 'f', 'lou', 'r</w>', 'or</w>', 'the</w>', 'mi', 'x', 'es</w>', 'i</w>', 'p', 'u', 'r', 'chase', 'd.</w>', 'i</w>', 'was</w>', 'about</w>', 'to</w>', 'g', 'i', 've</w>', 'u', 'p</w>', 'on</w>', 'f', 'in', 'ding</w>', 'a</w>', 'goo', 'd</w>', 'whe', 'at</w>', 'coo', 'k', 'i', 'e</w>', 'su', 'b', 'stitu', 'te</w>', 'and</w>', 'de', 'c', 'i', 'ded</w>', 'to</w>', 't', 'r', 'y</w>', 'these', ',</w>', 'ev', 'en</w>', 'th', 'ou', 'gh</w>', 'the</w>', 'p', 'ac', 'k', 'a', 'g', 'ing</w>', 'di', 'd</w>', 'not</w>', 'ma', 'ke</w>', 'the', 'm</w>', 'loo', 'k</w>', 'a', 'p', 'pe', 'ti', 'z', 'ing</w>', 'at</w>', 'al', 'l', '.</w>', 'to</w>', 'my</w>', 'su', 'r', 'p', 'ri', 'se</w>', 'these</w>', 'coo', 'k', 'i', 'es</w>', 'tasted</w>', 'like</w>', 'the</w>', 're', 'al</w>', 'thing', '.</w>', 'i</w>', 'am</w>', 'very</w>', 'ha', 'p', 'p', 'y</w>', 'and</w>', 'w', 'il', 'l</w>', 'con', 'tin', 'u', 'e</w>', 'p', 'u', 'r', 'chasing</w>', 'these</w>', 'from</w>', 'n', 'ow</w>', 'on</w>', 'whenever</w>', 'i</w>', 'fe', 'el</w>', 'like</w>', 'i</w>', 'ne', 'ed</w>', 'a</w>', 't', 're', 'at</w>', 'and</w>', 'w', 'il', 'l</w>', 'not</w>', 'fe', 'el</w>', 's', 'or', 'r', 'y</w>', 'for</w>', 'm', 'y', 'sel', 'f</w>', 'an', 'y</w>', 'more', '.</w>', 'than', 'k', 's</w>', 'p', 'amela', '!', '!</w>']\n",
      "------------------------------------------------------------\n",
      "@cyounes √ô?√ò¬π√ô¬Ñ√ò¬ß√ô¬ã √ô¬á√ò¬≤√ô¬ä√ô¬Ö√ò¬© √ò¬≥√ò¬ß√ò¬≠√ô¬Ç√ò¬© \n",
      "['@', 'c', 'younes</w>', '√π', '?', '√∏', '¬π', '√π', '\\x84', '√∏', '¬ß', '√π', '\\x8b', '</w>', '√π', '\\x87', '√∏', '¬≤', '√π', '\\x8a', '√π', '</w>', '√∏', '¬©', '</w>', '√∏', '¬≥', '√∏', '¬ß', '√∏', '\\xad', '√π', '\\x82', '√∏', '¬©', '</w>']\n",
      "------------------------------------------------------------\n",
      "Oh no ChatGPT! Academic dishonesty! I don't give a shit!\n",
      "['o', 'h', '</w>', 'n', 'o</w>', 'chatg', 'p', 't', '!</w>', 'acade', 'mic', '</w>', 'di', 'sh', 'onest', 'y', '!</w>', 'i</w>', \"don't</w>\", 'g', 'i', 've</w>', 'a</w>', 'shit', '!</w>']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "texts = df[text_col].astype(str).tolist()\n",
    "\n",
    "tokenizer = BPETokenizer(vocab_size=200)\n",
    "\n",
    "print(\"üîπ Training BPE tokenizer...\")\n",
    "tokenizer.train(texts)\n",
    "\n",
    "with open(\"tokenizer/subword_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenizer.vocab, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Vocabulary saved to tokenizer/subword_vocab.json\")\n",
    "\n",
    "print(\"\\nüîπ Sample Tokenization:\\n\")\n",
    "for i in range(10):\n",
    "    print(texts[i])\n",
    "    print(tokenizer.tokenize(texts[i]))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976e60de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Training custom embeddings...\n",
      "Epoch 1 completed\n",
      "Epoch 2 completed\n",
      "Epoch 3 completed\n",
      "Epoch 4 completed\n",
      "Epoch 5 completed\n",
      "‚úÖ Custom embeddings saved\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM = 50\n",
    "EPOCHS = 5\n",
    "LR = 0.01\n",
    "\n",
    "word2idx = tokenizer.vocab\n",
    "embeddings_custom = np.random.randn(len(word2idx), EMBED_DIM)\n",
    "\n",
    "print(\"üîπ Training custom embeddings...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for text in texts:\n",
    "        for word in str(text).lower().split():\n",
    "            if word in word2idx:\n",
    "                embeddings_custom[word2idx[word]] += LR\n",
    "    print(f\"Epoch {epoch+1} completed\")\n",
    "\n",
    "with open(\"embeddings/custom_embeddings.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word, idx in word2idx.items():\n",
    "        vec = \" \".join(map(str, embeddings_custom[idx]))\n",
    "        f.write(f\"{word} {vec}\\n\")\n",
    "\n",
    "print(\"‚úÖ Custom embeddings saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "792092b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    emb = {}\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            emb[parts[0]] = np.array(parts[1:], dtype=float)\n",
    "    return emb\n",
    "\n",
    "emb_custom = load_embeddings(\"embeddings/custom_embeddings.txt\")\n",
    "EMBED_DIM = len(next(iter(emb_custom.values())))\n",
    "\n",
    "def sentence_vector(text, emb):\n",
    "    vecs = [emb[w] for w in str(text).lower().split() if w in emb]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(EMBED_DIM)\n",
    "\n",
    "X_custom = np.array([sentence_vector(t, emb_custom) for t in df[text_col]])\n",
    "y = df[label_col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ecf1197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RESULTS ‚Äî Custom Embeddings\n",
      "üéØ Accuracy: 0.5810625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.55      0.57      8063\n",
      "           1       0.57      0.61      0.59      7937\n",
      "\n",
      "    accuracy                           0.58     16000\n",
      "   macro avg       0.58      0.58      0.58     16000\n",
      "weighted avg       0.58      0.58      0.58     16000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\Sentiment-Analysis-Subword-ML\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_custom, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_custom = LogisticRegression(max_iter=1000)\n",
    "model_custom.fit(X_train, y_train)\n",
    "\n",
    "preds_custom = model_custom.predict(X_test)\n",
    "\n",
    "print(\"‚úÖ RESULTS ‚Äî Custom Embeddings\")\n",
    "print(\"üéØ Accuracy:\", accuracy_score(y_test, preds_custom))\n",
    "print(classification_report(y_test, preds_custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cfde5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading GloVe embeddings...\n"
     ]
    }
   ],
   "source": [
    "def load_glove(path):\n",
    "    emb = {}\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            emb[values[0]] = np.array(values[1:], dtype=float)\n",
    "    return emb\n",
    "\n",
    "print(\"üîπ Loading GloVe embeddings...\")\n",
    "emb_glove = load_glove(\"embeddings/glove.6B.50d.txt\")\n",
    "EMBED_DIM = len(next(iter(emb_glove.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0debe88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RESULTS ‚Äî GloVe Embeddings\n",
      "üéØ Accuracy: 0.6755625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68      8063\n",
      "           1       0.67      0.67      0.67      7937\n",
      "\n",
      "    accuracy                           0.68     16000\n",
      "   macro avg       0.68      0.68      0.68     16000\n",
      "weighted avg       0.68      0.68      0.68     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_glove = np.array([sentence_vector(t, emb_glove) for t in df[text_col]])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_glove, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_glove = LogisticRegression(max_iter=1000)\n",
    "model_glove.fit(X_train, y_train)\n",
    "\n",
    "preds_glove = model_glove.predict(X_test)\n",
    "\n",
    "print(\"‚úÖ RESULTS ‚Äî GloVe Embeddings\")\n",
    "print(\"üéØ Accuracy:\", accuracy_score(y_test, preds_glove))\n",
    "print(classification_report(y_test, preds_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2a7f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä FINAL COMPARISON SUMMARY\n",
      "----------------------------------------\n",
      "Custom Embeddings Accuracy : 0.5810625\n",
      "GloVe Embeddings Accuracy  : 0.6755625\n",
      "----------------------------------------\n",
      "\n",
      "Conclusion:\n",
      "The model using pre-trained GloVe embeddings performs better than the\n",
      "custom embeddings trained from scratch. This is expected because GloVe\n",
      "embeddings are trained on very large corpora and capture richer semantic\n",
      "relationships. However, the custom pipeline successfully demonstrates\n",
      "the complete NLP workflow from tokenizer training to classification.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä FINAL COMPARISON SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Custom Embeddings Accuracy :\", accuracy_score(y_test, preds_custom))\n",
    "print(\"GloVe Embeddings Accuracy  :\", accuracy_score(y_test, preds_glove))\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\"\"\n",
    "Conclusion:\n",
    "The model using pre-trained GloVe embeddings performs better than the\n",
    "custom embeddings trained from scratch. This is expected because GloVe\n",
    "embeddings are trained on very large corpora and capture richer semantic\n",
    "relationships. However, the custom pipeline successfully demonstrates\n",
    "the complete NLP workflow from tokenizer training to classification.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
